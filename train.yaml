
experiment_name:
  Auto_config
mult_k: 2
set_seed:
  False
pytorch_seed:
  2021
data:
  cifar100
use_standard:
  True
set_share_layer:
  False
parameters:
  # default parameters
  s:
    4
  push_num:
    5
  length:
    25
  tmp_feature:
    64
  sigma:
    0.1
  batch_size:
    64
  epoch:
    200
  dropout:
    0.5
  filter_tau_m:
    4
  filter_tau_s:
    1
  train_coefficients:
    True
  train_bias:
    True
  membrane_filer:
    False
shape: [[2,2,2]]
optimizer:
  optimizer_choice:
    'SGD'
  Adam:
    lr: 0.005
    weight_decay: 0
  AdamW:
    lr: 0.005
    weight_decay: 1e-4
  SAM:
    lr: 0.002
    weight_decay: 1e-4
  SGD:
    momentum: 0.7
    lr: 0.005
    weight_decay: 1e-4
  ASGD:
    lr: 0.0001
    weight_decay: 1e-4
  Rprop:
    lr: 0.0001
    etas: (0.5,1.5)


scheduler:
  scheduler_choice:
    'SchedulerLR'
  SchedulerLR:
    milestones:
      - 0.4
      - 0.7
    gamma:
      0.1
  MultiStepLR:
    milestones:
      - 25
      - 35
    gamma:
      0.1
  CosineAnnealingWarmRestarts:
    T_0:
      1000
  CyclicLR:
    base_lr:
      0.0001
    max_lr:
      0.001
    step_size_up:
      2000

transform:
  RandomResizedCrop:
    size:
      28
    scale:
      - 0.85
      - 1.0
    ratio:
      - 0.75
      - 1.3333333333333333
    interpolation:
      2
  RandomRotation:
    angle:
      15
  RandomApply:
    probability:
      0.5

mnist_config:
  max_rate:
    1
  use_transform:
    True
output:
  ./output